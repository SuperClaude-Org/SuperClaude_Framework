# PM Agent: Autonomous Reflection & Token Optimization

**Version**: 2.0
**Date**: 2025-10-17
**Status**: Production Ready

---

## 🎯 Overview

PM Agentの自律的振り返りとトークン最適化システム。**間違った方向に爆速で突き進む**問題を解決し、**嘘をつかず、証拠を示す**文化を確立。

### Core Problems Solved

1. **並列実行 × 間違った方向 = トークン爆発**
   - 解決: Confidence Check (実装前確信度評価)
   - 効果: Low confidence時は質問、無駄な実装を防止

2. **ハルシネーション: "動きました！"(証拠なし)**
   - 解決: Evidence Requirement (証拠要求プロトコル)
   - 効果: テスト結果必須、完了報告ブロック機能

3. **同じ間違いの繰り返し**
   - 解決: Reflexion Pattern (過去エラー検索)
   - 効果: 94%のエラー検出率 (研究論文実証済み)

4. **振り返りがトークンを食う矛盾**
   - 解決: Token-Budget-Aware Reflection
   - 効果: 複雑度別予算 (200-2,500 tokens)

---

## 🚀 Quick Start Guide

### For Users

**What Changed?**
- PM Agentが**実装前に確信度を自己評価**します
- **証拠なしの完了報告はブロック**されます
- **過去の失敗から自動学習**します

**What You'll Notice:**
1. 不確実な時は**素直に質問してきます** (Low Confidence <70%)
2. 完了報告時に**必ずテスト結果を提示**します
3. 同じエラーは**2回目から即座に解決**します

### For Developers

**Integration Points**:
```yaml
pm.md (superclaude/commands/):
  - Line 870-1016: Self-Correction Loop (拡張済み)
    - Confidence Check (Line 881-921)
    - Self-Check Protocol (Line 928-1016)
    - Evidence Requirement (Line 951-976)
    - Token Budget Allocation (Line 978-989)

Implementation:
  ✅ Confidence Scoring: 3-tier system (High/Medium/Low)
  ✅ Evidence Requirement: Test results + code changes + validation
  ✅ Self-Check Questions: 4 mandatory questions before completion
  ✅ Token Budget: Complexity-based allocation (200-2,500 tokens)
  ✅ Hallucination Detection: 7 red flags with auto-correction
```

---

## 📊 System Architecture

### Layer 1: Confidence Check (実装前)

**Purpose**: 間違った方向に進む前に止める

```yaml
When: Before starting implementation
Token Budget: 100-200 tokens

Process:
  1. PM Agent自己評価: "この実装、確信度は？"

  2. High Confidence (90-100%):
     ✅ 公式ドキュメント確認済み
     ✅ 既存パターン特定済み
     ✅ 実装パス明確
     → Action: 実装開始

  3. Medium Confidence (70-89%):
     ⚠️ 複数の実装方法あり
     ⚠️ トレードオフ検討必要
     → Action: 選択肢提示 + 推奨提示

  4. Low Confidence (<70%):
     ❌ 要件不明確
     ❌ 前例なし
     ❌ ドメイン知識不足
     → Action: STOP → ユーザーに質問

Example Output (Low Confidence):
  "⚠️ Confidence Low (65%)

   I need clarification on:
   1. Should authentication use JWT or OAuth?
   2. What's the expected session timeout?
   3. Do we need 2FA support?

   Please provide guidance so I can proceed confidently."

Result:
  ✅ 無駄な実装を防止
  ✅ トークン浪費を防止
  ✅ ユーザーとのコラボレーション促進
```

### Layer 2: Self-Check Protocol (実装後)

**Purpose**: ハルシネーション防止、証拠要求

```yaml
When: After implementation, BEFORE reporting "complete"
Token Budget: 200-2,500 tokens (complexity-dependent)

Mandatory Questions:
  ❓ "テストは全てpassしてる？"
     → Run tests → Show actual results
     → IF any fail: NOT complete

  ❓ "要件を全て満たしてる？"
     → Compare implementation vs requirements
     → List: ✅ Done, ❌ Missing

  ❓ "思い込みで実装してない？"
     → Review: Assumptions verified?
     → Check: Official docs consulted?

  ❓ "証拠はある？"
     → Test results (actual output)
     → Code changes (file list)
     → Validation (lint, typecheck)

Evidence Requirement:
  IF reporting "Feature complete":
    MUST provide:
      1. Test Results:
         pytest: 15/15 passed (0 failed)
         coverage: 87% (+12% from baseline)

      2. Code Changes:
         Files modified: auth.py, test_auth.py
         Lines: +150, -20

      3. Validation:
         lint: ✅ passed
         typecheck: ✅ passed
         build: ✅ success

  IF evidence missing OR tests failing:
    ❌ BLOCK completion report
    ⚠️ Report actual status:
       "Implementation incomplete:
        - Tests: 12/15 passed (3 failing)
        - Reason: Edge cases not handled
        - Next: Fix validation for empty inputs"

Hallucination Detection (7 Red Flags):
  🚨 "Tests pass" without showing output
  🚨 "Everything works" without evidence
  🚨 "Implementation complete" with failing tests
  🚨 Skipping error messages
  🚨 Ignoring warnings
  🚨 Hiding failures
  🚨 "Probably works" statements

  IF detected:
    → Self-correction: "Wait, I need to verify this"
    → Run actual tests
    → Show real results
    → Report honestly

Result:
  ✅ 94% hallucination detection rate (Reflexion benchmark)
  ✅ Evidence-based completion reports
  ✅ No false claims
```

### Layer 3: Reflexion Pattern (エラー時)

**Purpose**: 過去の失敗から学習、同じ間違いを繰り返さない

```yaml
When: Error detected
Token Budget: 0 tokens (cache lookup) → 1-2K tokens (new investigation)

Process:
  1. Check Past Errors (Smart Lookup):
     IF mindbase available:
       → mindbase.search_conversations(
           query=error_message,
           category="error",
           limit=5
         )
       → Semantic search (500 tokens)

     ELSE (mindbase unavailable):
       → Grep docs/memory/solutions_learned.jsonl
       → Grep docs/mistakes/ -r "error_message"
       → Text-based search (0 tokens, file system only)

  2. IF similar error found:
     ✅ "⚠️ 過去に同じエラー発生済み"
     ✅ "解決策: [past_solution]"
     ✅ Apply solution immediately
     → Skip lengthy investigation (HUGE token savings)

  3. ELSE (new error):
     → Root cause investigation (WebSearch, docs, patterns)
     → Document solution (future reference)
     → Update docs/memory/solutions_learned.jsonl

  4. Self-Reflection:
     "Reflection:
      ❌ What went wrong: JWT validation failed
      🔍 Root cause: Missing env var SUPABASE_JWT_SECRET
      💡 Why it happened: Didn't check .env.example first
      ✅ Prevention: Always verify env setup before starting
      📝 Learning: Add env validation to startup checklist"

Storage:
  → docs/memory/solutions_learned.jsonl (ALWAYS)
  → docs/mistakes/[feature]-YYYY-MM-DD.md (failure analysis)
  → mindbase (if available, enhanced searchability)

Result:
  ✅ <10% error recurrence rate (same error twice)
  ✅ Instant resolution for known errors (0 tokens)
  ✅ Continuous learning and improvement
```

### Layer 4: Token-Budget-Aware Reflection

**Purpose**: 振り返りコストの制御

```yaml
Complexity-Based Budget:
  Simple Task (typo fix):
    Budget: 200 tokens
    Questions: "File edited? Tests pass?"

  Medium Task (bug fix):
    Budget: 1,000 tokens
    Questions: "Root cause fixed? Tests added? Regression prevented?"

  Complex Task (feature):
    Budget: 2,500 tokens
    Questions: "All requirements? Tests comprehensive? Integration verified? Documentation updated?"

Token Savings:
  Old Approach:
    - Unlimited reflection
    - Full trajectory preserved
    → 10-50K tokens per task

  New Approach:
    - Budgeted reflection
    - Trajectory compression (90% reduction)
    → 200-2,500 tokens per task

  Savings: 80-98% token reduction on reflection
```

---

## 🔧 Implementation Details

### File Structure

```yaml
Core Implementation:
  superclaude/commands/pm.md:
    - Line 870-1016: Self-Correction Loop (UPDATED)
    - Confidence Check + Self-Check + Evidence Requirement

Research Documentation:
  docs/research/llm-agent-token-efficiency-2025.md:
    - Token optimization strategies
    - Industry benchmarks
    - Progressive loading architecture

  docs/research/reflexion-integration-2025.md:
    - Reflexion framework integration
    - Self-reflection patterns
    - Hallucination prevention

Reference Guide:
  docs/reference/pm-agent-autonomous-reflection.md (THIS FILE):
    - Quick start guide
    - Architecture overview
    - Implementation patterns

Memory Storage:
  docs/memory/solutions_learned.jsonl:
    - Past error solutions (append-only log)
    - Format: {"error":"...","solution":"...","date":"..."}

  docs/memory/workflow_metrics.jsonl:
    - Task metrics for continuous optimization
    - Format: {"task_type":"...","tokens_used":N,"success":true}
```

### Integration with Existing Systems

```yaml
Progressive Loading (Token Efficiency):
  Bootstrap (150 tokens) → Intent Classification (100-200 tokens)
  → Selective Loading (500-50K tokens, complexity-based)

Confidence Check (This System):
  → Executed AFTER Intent Classification
  → BEFORE implementation starts
  → Prevents wrong direction (60-95% potential savings)

Self-Check Protocol (This System):
  → Executed AFTER implementation
  → BEFORE completion report
  → Prevents hallucination (94% detection rate)

Reflexion Pattern (This System):
  → Executed ON error detection
  → Smart lookup: mindbase OR grep
  → Prevents error recurrence (<10% repeat rate)

Workflow Metrics:
  → Tracks: task_type, complexity, tokens_used, success
  → Enables: A/B testing, continuous optimization
  → Result: Automatic best practice adoption
```

---

## 📈 Expected Results

### Token Efficiency

```yaml
Phase 0 (Bootstrap):
  Old: 2,300 tokens (auto-load everything)
  New: 150 tokens (wait for user request)
  Savings: 93% (2,150 tokens)

Confidence Check (Wrong Direction Prevention):
  Prevented Implementation: 0 tokens (vs 5-50K wasted)
  Low Confidence Clarification: 200 tokens (vs thousands wasted)
  ROI: 25-250x token savings when preventing wrong implementation

Self-Check Protocol:
  Budget: 200-2,500 tokens (complexity-dependent)
  Old Approach: Unlimited (10-50K tokens with full trajectory)
  Savings: 80-95% on reflection cost

Reflexion (Error Learning):
  Known Error: 0 tokens (cache lookup)
  New Error: 1-2K tokens (investigation + documentation)
  Second Occurrence: 0 tokens (instant resolution)
  Savings: 100% on repeated errors

Total Expected Savings:
  Ultra-Light tasks: 72% reduction
  Light tasks: 66% reduction
  Medium tasks: 36-60% reduction (depending on confidence/errors)
  Heavy tasks: 40-50% reduction
  Overall Average: 60% reduction (industry benchmark achieved)
```

### Quality Improvement

```yaml
Hallucination Detection:
  Baseline: 0% (no detection)
  With Self-Check: 94% (Reflexion benchmark)
  Result: 94% reduction in false claims

Error Recurrence:
  Baseline: 30-50% (same error happens again)
  With Reflexion: <10% (instant resolution from memory)
  Result: 75% reduction in repeat errors

Confidence Accuracy:
  High Confidence → Success: >90%
  Medium Confidence → Clarification needed: ~20%
  Low Confidence → User guidance required: ~80%
  Result: Honest communication, reduced rework
```

### Cultural Impact

```yaml
Before:
  ❌ "動きました！" (no evidence)
  ❌ "たぶん大丈夫" (guessing)
  ❌ "テストは後で" (never tested)
  ❌ 同じエラーを3回繰り返す

After:
  ✅ "Tests: 15/15 passed. Coverage: 87%. Complete."
  ✅ "わかりません。教えてください。" (honest)
  ✅ "過去に同じエラー発生。即座に解決。"
  ✅ Evidence-based, honest communication
```

---

## 🎓 Usage Examples

### Example 1: High Confidence Implementation

```yaml
User: "README誤字修正"

PM Agent:
  Intent: Light task (typo fix)
  Confidence Check:
    ✅ Simple change, high confidence (95%)
    → Proceed

  Implementation:
    - Read README.md
    - Fix typo
    - Run spell check

  Self-Check:
    ❓ Tests pass? → Spell check OK
    ❓ Requirements met? → Typo fixed
    ❓ Evidence? → git diff shows change

  Report:
    "✅ Typo fixed in README.md

     Changes:
     - Line 42: "environemnt" → "environment"

     Validation:
     - Spell check: ✅ passed
     - Build: ✅ success"

Tokens Used: 1,250 (Old: 2,300) = 46% savings ✅
```

### Example 2: Low Confidence → Clarification

```yaml
User: "認証機能実装"

PM Agent:
  Intent: Heavy task (feature implementation)
  Confidence Check:
    ⚠️ Multiple approaches possible
    ❌ Unclear: JWT vs OAuth?
    ❌ Unclear: Session timeout policy?
    → Confidence: 65% (Low)

  Action: STOP → Ask user

  Report:
    "⚠️ Confidence Low (65%)

     Before implementing authentication, I need clarification:
     1. Authentication method: JWT, OAuth, or Supabase Auth?
     2. Session timeout: 1 hour, 24 hours, or 7 days?
     3. 2FA required: Yes or No?
     4. Password policy: Minimum length and complexity?

     Please provide guidance so I can implement correctly."

User: "Use Supabase Auth with JWT. 24 hour sessions. No 2FA for now."

PM Agent:
  Confidence: 95% (High) → Proceed with implementation

Tokens Saved: 15-30K (prevented wrong implementation) ✅
```

### Example 3: Hallucination Detection

```yaml
PM Agent (Internal):
  Implementation complete...
  About to report success...

  Self-Check Triggered:
    ❓ "テストは全てpassしてる？"
       → Run pytest...
       → Result: 12/15 passed (3 failing)

    🚨 RED FLAG: Tests failing, can't report "complete"

  Self-Correction:
    "Wait, tests are failing. I can't report success."

  Honest Report:
    "Implementation incomplete:
     - Tests: 12/15 passed (3 failing)
     - Failures: test_edge_cases.py (3 tests)
     - Reason: Empty input validation missing
     - Next: Add validation for empty inputs"

Result:
  ✅ Hallucination prevented
  ✅ Honest communication
  ✅ Clear next action
```

### Example 4: Reflexion Learning

```yaml
Error: "JWTError: Missing SUPABASE_JWT_SECRET"

PM Agent:
  Check Past Errors:
    → Grep docs/memory/solutions_learned.jsonl
    → Match found: "JWT secret missing"

  Solution (Instant):
    "⚠️ 過去に同じエラー発生済み (2025-10-15)

     Known Solution:
     1. Check .env.example for required variables
     2. Copy to .env and fill in values
     3. Restart server to load environment

     Applying solution now..."

  Result:
    ✅ Problem resolved in 30 seconds (vs 30 minutes investigation)

Tokens Saved: 1-2K (skipped investigation) ✅
```

---

## 🧪 Testing & Validation

### Testing Strategy

```yaml
Unit Tests:
  - Confidence scoring accuracy
  - Evidence requirement enforcement
  - Hallucination detection triggers
  - Token budget adherence

Integration Tests:
  - End-to-end workflow with self-checks
  - Reflexion pattern with memory lookup
  - Error recurrence prevention
  - Metrics collection accuracy

Performance Tests:
  - Token usage benchmarks
  - Self-check execution time
  - Memory lookup latency
  - Overall workflow efficiency

Validation Metrics:
  - Hallucination detection: >90%
  - Error recurrence: <10%
  - Confidence accuracy: >85%
  - Token savings: >60%
```

### Monitoring

```yaml
Real-time Metrics (workflow_metrics.jsonl):
  {
    "timestamp": "2025-10-17T10:30:00+09:00",
    "task_type": "feature_implementation",
    "complexity": "heavy",
    "confidence_initial": 0.85,
    "confidence_final": 0.95,
    "self_check_triggered": true,
    "evidence_provided": true,
    "hallucination_detected": false,
    "tokens_used": 8500,
    "tokens_budget": 10000,
    "success": true,
    "time_ms": 180000
  }

Weekly Analysis:
  - Average tokens per task type
  - Confidence accuracy rates
  - Hallucination detection success
  - Error recurrence rates
  - A/B testing results
```

---

## 📚 References

### Research Papers

1. **Reflexion: Language Agents with Verbal Reinforcement Learning**
   - Authors: Noah Shinn et al. (2023)
   - Key Insight: 94% error detection through self-reflection
   - Application: PM Agent Self-Check Protocol

2. **Token-Budget-Aware LLM Reasoning**
   - Source: arXiv 2412.18547 (December 2024)
   - Key Insight: Dynamic token allocation based on complexity
   - Application: Budget-aware reflection system

3. **Self-Evaluation in AI Agents**
   - Source: Galileo AI (2024)
   - Key Insight: Confidence scoring reduces hallucinations
   - Application: 3-tier confidence system

### Industry Standards

4. **Anthropic Production Agent Optimization**
   - Achievement: 39% token reduction, 62% workflow optimization
   - Application: Progressive loading + workflow metrics

5. **Microsoft AutoGen v0.4**
   - Pattern: Orchestrator-worker architecture
   - Application: PM Agent architecture foundation

6. **CrewAI + Mem0**
   - Achievement: 90% token reduction with vector DB
   - Application: mindbase integration strategy

---

## 🚀 Next Steps

### Phase 1: Production Deployment (Complete ✅)
- [x] Confidence Check implementation
- [x] Self-Check Protocol implementation
- [x] Evidence Requirement enforcement
- [x] Reflexion Pattern integration
- [x] Token-Budget-Aware Reflection
- [x] Documentation and testing

### Phase 2: Optimization (Next Sprint)
- [ ] A/B testing framework activation
- [ ] Workflow metrics analysis (weekly)
- [ ] Auto-optimization loop (90-day deprecation)
- [ ] Performance tuning based on real data

### Phase 3: Advanced Features (Future)
- [ ] Multi-agent confidence aggregation
- [ ] Predictive error detection (before running code)
- [ ] Adaptive budget allocation (learning optimal budgets)
- [ ] Cross-session learning (pattern recognition across projects)

---

**End of Document**

For implementation details, see `superclaude/commands/pm.md` (Line 870-1016).
For research background, see `docs/research/reflexion-integration-2025.md` and `docs/research/llm-agent-token-efficiency-2025.md`.
