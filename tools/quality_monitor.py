#!/usr/bin/env python3
"""
SuperClaudeç¿»è¯‘è´¨é‡ç›‘æ§ç³»ç»Ÿ / SuperClaude Translation Quality Monitoring System
å®æ—¶ç›‘æ§ç¿»è¯‘è´¨é‡ï¼Œç”Ÿæˆè´¨é‡æŠ¥å‘Šå’Œæ”¹è¿›å»ºè®® / Real-time monitoring of translation quality, generating quality reports and improvement suggestions
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from i18n.cache import TranslationCache
    from i18n.validator import QualityValidator
    from i18n.incremental import IncrementalTranslationManager
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥è­¦å‘Š: {e}")


@dataclass
class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡ / Quality Metrics"""
    language: str
    total_items: int
    quality_score: float
    consistency_score: float
    completeness_score: float
    accuracy_score: float
    fluency_score: float
    build_time: str
    file_size_kb: float
    last_updated: str


@dataclass
class QualityTrend:
    """è´¨é‡è¶‹åŠ¿ / Quality Trend"""
    language: str
    quality_history: List[Tuple[str, float]]  # (timestamp, quality_score)
    trend_direction: str  # "improving", "declining", "stable"
    trend_strength: float  # 0.0-1.0
    recommendation: str


@dataclass
class QualityAlert:
    """è´¨é‡å‘Šè­¦ / Quality Alert"""
    severity: str  # "low", "medium", "high", "critical"
    language: str
    metric: str
    current_value: float
    threshold_value: float
    message: str
    timestamp: str


class TranslationQualityMonitor:
    """ç¿»è¯‘è´¨é‡ç›‘æ§å™¨ / Translation Quality Monitor"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.locale_dir = self.project_root / "i18n" / "locales"
        self.monitor_data_dir = self.project_root / ".superclaude" / "quality_monitor"
        self.monitor_data_dir.mkdir(parents=True, exist_ok=True)
        
        # è´¨é‡å†å²æ–‡ä»¶
        self.quality_history_file = self.monitor_data_dir / "quality_history.jsonl"
        self.alerts_file = self.monitor_data_dir / "quality_alerts.json"
        self.config_file = self.monitor_data_dir / "monitor_config.json"
        
        # åŠ è½½é…ç½®
        self.config = self._load_monitor_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.validator = QualityValidator()
        self.cache = TranslationCache()
        
    def _load_monitor_config(self) -> Dict[str, Any]:
        """åŠ è½½ç›‘æ§é…ç½®"""
        default_config = {
            "quality_thresholds": {
                "critical": 0.6,    # ä½äºæ­¤åˆ†æ•°ä¸ºä¸¥é‡é—®é¢˜
                "warning": 0.8,     # ä½äºæ­¤åˆ†æ•°ä¸ºè­¦å‘Š
                "target": 0.95      # ç›®æ ‡è´¨é‡åˆ†æ•°
            },
            "completeness_thresholds": {
                "critical": 0.7,    # å®Œæ•´åº¦ä½äº70%ä¸ºä¸¥é‡é—®é¢˜
                "warning": 0.9,     # å®Œæ•´åº¦ä½äº90%ä¸ºè­¦å‘Š
                "target": 1.0       # ç›®æ ‡å®Œæ•´åº¦100%
            },
            "trend_analysis": {
                "history_days": 30, # åˆ†ææœ€è¿‘30å¤©çš„è¶‹åŠ¿
                "min_data_points": 3 # è‡³å°‘3ä¸ªæ•°æ®ç‚¹æ‰åˆ†æè¶‹åŠ¿
            },
            "alert_settings": {
                "max_alerts": 100,  # æœ€å¤§å‘Šè­¦æ•°é‡
                "alert_cooldown": 3600 # ç›¸åŒå‘Šè­¦çš„å†·å´æ—¶é—´ï¼ˆç§’ï¼‰
            },
            "monitored_languages": ["zh_CN", "ja_JP", "ko_KR", "ru_RU", "es_ES", "de_DE", "fr_FR", "ar_SA"]
        }
        
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    # åˆå¹¶ç”¨æˆ·é…ç½®å’Œé»˜è®¤é…ç½®
                    default_config.update(user_config)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ç›‘æ§é…ç½®å¤±è´¥: {e}")
        
        return default_config
    
    def _save_monitor_config(self):
        """ä¿å­˜ç›‘æ§é…ç½®"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç›‘æ§é…ç½®å¤±è´¥: {e}")
    
    def _calculate_quality_metrics(self, locale_file: Path) -> Optional[QualityMetrics]:
        """è®¡ç®—å•ä¸ªè¯­è¨€çš„è´¨é‡æŒ‡æ ‡"""
        if not locale_file.exists():
            return None
        
        try:
            with open(locale_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            metadata = data.get('metadata', {})
            language = metadata.get('language', locale_file.stem)
            
            # åŸºç¡€æŒ‡æ ‡
            total_items = metadata.get('total_items', 0)
            quality_score = metadata.get('quality_score', 0.0)
            build_time = metadata.get('build_time', '')
            
            # è®¡ç®—å®Œæ•´åº¦
            expected_sections = ['commands', 'personas', 'ui', 'errors', 'help']
            completeness_score = 0.0
            
            if total_items > 0:
                existing_items = 0
                for section in expected_sections:
                    if section in data and isinstance(data[section], dict):
                        existing_items += len([v for v in data[section].values() if v.strip()])
                
                completeness_score = existing_items / total_items if total_items > 0 else 0
            
            # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°ï¼ˆåŸºäºæœ¯è¯­ä½¿ç”¨ï¼‰
            consistency_score = self._calculate_consistency_score(data)
            
            # å‡†ç¡®æ€§å’Œæµç•…æ€§è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            accuracy_score = min(quality_score * 1.1, 1.0)  # åŸºäºè´¨é‡åˆ†æ•°ä¼°ç®—
            fluency_score = min(quality_score * 1.05, 1.0)   # åŸºäºè´¨é‡åˆ†æ•°ä¼°ç®—
            
            # æ–‡ä»¶å¤§å°
            file_size_kb = locale_file.stat().st_size / 1024
            
            return QualityMetrics(
                language=language,
                total_items=total_items,
                quality_score=quality_score,
                consistency_score=consistency_score,
                completeness_score=completeness_score,
                accuracy_score=accuracy_score,
                fluency_score=fluency_score,
                build_time=build_time,
                file_size_kb=round(file_size_kb, 2),
                last_updated=datetime.now().isoformat()
            )
            
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—è´¨é‡æŒ‡æ ‡å¤±è´¥ {locale_file}: {e}")
            return None
    
    def _calculate_consistency_score(self, data: Dict[str, Any]) -> float:
        """è®¡ç®—ç¿»è¯‘ä¸€è‡´æ€§åˆ†æ•°"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ä¸€è‡´æ€§æ£€æŸ¥é€»è¾‘
        # ç›®å‰ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        
        try:
            # æ£€æŸ¥å…³é”®æœ¯è¯­çš„ä¸€è‡´æ€§
            key_terms = {
                "SuperClaude", "Claude", "AI", "API", "Git", "GitHub",
                "å‘½ä»¤", "åˆ†æ", "å®ç°", "æ„å»º", "ç¿»è¯‘", "è´¨é‡"
            }
            
            all_text = []
            for section in ['commands', 'personas', 'ui', 'errors', 'help']:
                if section in data and isinstance(data[section], dict):
                    all_text.extend(data[section].values())
            
            if not all_text:
                return 0.8  # é»˜è®¤åˆ†æ•°
            
            # ç®€å•çš„ä¸€è‡´æ€§è¯„åˆ†
            text_content = " ".join(all_text).lower()
            consistency_indicators = 0
            total_checks = 0
            
            for term in key_terms:
                if term.lower() in text_content:
                    # æ£€æŸ¥æœ¯è¯­ä½¿ç”¨çš„ä¸€è‡´æ€§ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                    count = text_content.count(term.lower())
                    if count > 0:
                        consistency_indicators += min(count / len(all_text), 1.0)
                        total_checks += 1
            
            consistency_score = consistency_indicators / total_checks if total_checks > 0 else 0.8
            return min(consistency_score, 1.0)
            
        except Exception:
            return 0.8  # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›é»˜è®¤åˆ†æ•°
    
    def collect_quality_metrics(self) -> Dict[str, QualityMetrics]:
        """æ”¶é›†æ‰€æœ‰è¯­è¨€çš„è´¨é‡æŒ‡æ ‡"""
        metrics = {}
        
        if not self.locale_dir.exists():
            print("âš ï¸ æœ¬åœ°åŒ–ç›®å½•ä¸å­˜åœ¨")
            return metrics
        
        monitored_languages = self.config.get('monitored_languages', [])
        
        for locale_file in self.locale_dir.glob('*.json'):
            if locale_file.name == 'index.json':
                continue
                
            language = locale_file.stem
            if monitored_languages and language not in monitored_languages:
                continue
            
            metric = self._calculate_quality_metrics(locale_file)
            if metric:
                metrics[language] = metric
        
        return metrics
    
    def _record_quality_history(self, metrics: Dict[str, QualityMetrics]):
        """è®°å½•è´¨é‡å†å²"""
        timestamp = datetime.now().isoformat()
        
        try:
            with open(self.quality_history_file, 'a', encoding='utf-8') as f:
                for language, metric in metrics.items():
                    history_entry = {
                        'timestamp': timestamp,
                        'language': language,
                        'quality_score': metric.quality_score,
                        'completeness_score': metric.completeness_score,
                        'consistency_score': metric.consistency_score,
                        'total_items': metric.total_items
                    }
                    f.write(json.dumps(history_entry, ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"âš ï¸ è®°å½•è´¨é‡å†å²å¤±è´¥: {e}")
    
    def _load_quality_history(self, language: str, days: int = 30) -> List[Tuple[str, float]]:
        """åŠ è½½è´¨é‡å†å²"""
        if not self.quality_history_file.exists():
            return []
        
        cutoff_date = datetime.now() - timedelta(days=days)
        history = []
        
        try:
            with open(self.quality_history_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        entry = json.loads(line)
                        if entry.get('language') == language:
                            entry_date = datetime.fromisoformat(entry['timestamp'])
                            if entry_date >= cutoff_date:
                                history.append((entry['timestamp'], entry['quality_score']))
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è´¨é‡å†å²å¤±è´¥: {e}")
        
        return sorted(history, key=lambda x: x[0])
    
    def analyze_quality_trends(self, metrics: Dict[str, QualityMetrics]) -> Dict[str, QualityTrend]:
        """åˆ†æè´¨é‡è¶‹åŠ¿"""
        trends = {}
        history_days = self.config['trend_analysis']['history_days']
        min_data_points = self.config['trend_analysis']['min_data_points']
        
        for language, current_metric in metrics.items():
            history = self._load_quality_history(language, history_days)
            
            if len(history) < min_data_points:
                # æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿
                trends[language] = QualityTrend(
                    language=language,
                    quality_history=history,
                    trend_direction="insufficient_data",
                    trend_strength=0.0,
                    recommendation="éœ€è¦æ›´å¤šå†å²æ•°æ®æ¥åˆ†æè¶‹åŠ¿"
                )
                continue
            
            # è®¡ç®—è¶‹åŠ¿
            scores = [score for _, score in history]
            
            # ç®€å•çš„è¶‹åŠ¿åˆ†æï¼šæ¯”è¾ƒå‰åŠæ®µå’ŒååŠæ®µçš„å¹³å‡åˆ†
            mid_point = len(scores) // 2
            early_avg = sum(scores[:mid_point]) / mid_point if mid_point > 0 else 0
            recent_avg = sum(scores[mid_point:]) / (len(scores) - mid_point)
            
            trend_change = recent_avg - early_avg
            trend_strength = abs(trend_change)
            
            if trend_change > 0.02:  # æå‡è¶…è¿‡2%
                trend_direction = "improving"
                recommendation = "è´¨é‡å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œç»§ç»­ä¿æŒ"
            elif trend_change < -0.02:  # ä¸‹é™è¶…è¿‡2%
                trend_direction = "declining"
                recommendation = "è´¨é‡å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦å…³æ³¨å’Œæ”¹è¿›"
            else:
                trend_direction = "stable"
                recommendation = "è´¨é‡ä¿æŒç¨³å®š"
            
            trends[language] = QualityTrend(
                language=language,
                quality_history=history,
                trend_direction=trend_direction,
                trend_strength=min(trend_strength * 10, 1.0),  # è§„èŒƒåŒ–åˆ°0-1
                recommendation=recommendation
            )
        
        return trends
    
    def generate_quality_alerts(self, metrics: Dict[str, QualityMetrics]) -> List[QualityAlert]:
        """ç”Ÿæˆè´¨é‡å‘Šè­¦"""
        alerts = []
        timestamp = datetime.now().isoformat()
        
        quality_thresholds = self.config['quality_thresholds']
        completeness_thresholds = self.config['completeness_thresholds']
        
        for language, metric in metrics.items():
            # è´¨é‡åˆ†æ•°å‘Šè­¦
            if metric.quality_score < quality_thresholds['critical']:
                alerts.append(QualityAlert(
                    severity="critical",
                    language=language,
                    metric="quality_score",
                    current_value=metric.quality_score,
                    threshold_value=quality_thresholds['critical'],
                    message=f"{language}ç¿»è¯‘è´¨é‡ä¸¥é‡åä½({metric.quality_score:.3f})",
                    timestamp=timestamp
                ))
            elif metric.quality_score < quality_thresholds['warning']:
                alerts.append(QualityAlert(
                    severity="medium",
                    language=language,
                    metric="quality_score",
                    current_value=metric.quality_score,
                    threshold_value=quality_thresholds['warning'],
                    message=f"{language}ç¿»è¯‘è´¨é‡éœ€è¦æ”¹è¿›({metric.quality_score:.3f})",
                    timestamp=timestamp
                ))
            
            # å®Œæ•´åº¦å‘Šè­¦
            if metric.completeness_score < completeness_thresholds['critical']:
                alerts.append(QualityAlert(
                    severity="critical",
                    language=language,
                    metric="completeness_score",
                    current_value=metric.completeness_score,
                    threshold_value=completeness_thresholds['critical'],
                    message=f"{language}ç¿»è¯‘å®Œæ•´åº¦ä¸¥é‡ä¸è¶³({metric.completeness_score:.1%})",
                    timestamp=timestamp
                ))
            elif metric.completeness_score < completeness_thresholds['warning']:
                alerts.append(QualityAlert(
                    severity="medium",
                    language=language,
                    metric="completeness_score",
                    current_value=metric.completeness_score,
                    threshold_value=completeness_thresholds['warning'],
                    message=f"{language}ç¿»è¯‘å®Œæ•´åº¦éœ€è¦æå‡({metric.completeness_score:.1%})",
                    timestamp=timestamp
                ))
            
            # ä¸€è‡´æ€§å‘Šè­¦
            if metric.consistency_score < 0.7:
                alerts.append(QualityAlert(
                    severity="medium",
                    language=language,
                    metric="consistency_score",
                    current_value=metric.consistency_score,
                    threshold_value=0.7,
                    message=f"{language}ç¿»è¯‘ä¸€è‡´æ€§éœ€è¦æ”¹è¿›({metric.consistency_score:.3f})",
                    timestamp=timestamp
                ))
        
        return alerts
    
    def _save_alerts(self, alerts: List[QualityAlert]):
        """ä¿å­˜å‘Šè­¦"""
        try:
            alerts_data = [asdict(alert) for alert in alerts]
            
            # åŠ è½½ç°æœ‰å‘Šè­¦
            existing_alerts = []
            if self.alerts_file.exists():
                with open(self.alerts_file, 'r', encoding='utf-8') as f:
                    existing_alerts = json.load(f)
            
            # åˆå¹¶å‘Šè­¦ï¼ˆé¿å…é‡å¤ï¼‰
            max_alerts = self.config['alert_settings']['max_alerts']
            all_alerts = alerts_data + existing_alerts
            
            # æŒ‰æ—¶é—´æ’åºå¹¶é™åˆ¶æ•°é‡
            all_alerts.sort(key=lambda x: x['timestamp'], reverse=True)
            all_alerts = all_alerts[:max_alerts]
            
            # ä¿å­˜å‘Šè­¦
            with open(self.alerts_file, 'w', encoding='utf-8') as f:
                json.dump(all_alerts, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å‘Šè­¦å¤±è´¥: {e}")
    
    def generate_quality_report(self, include_trends: bool = True, 
                               include_alerts: bool = True) -> Dict[str, Any]:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆç¿»è¯‘è´¨é‡æŠ¥å‘Š...")
        
        # æ”¶é›†æŒ‡æ ‡
        metrics = self.collect_quality_metrics()
        
        if not metrics:
            return {
                "status": "error",
                "message": "æœªæ‰¾åˆ°ç¿»è¯‘æ–‡ä»¶æˆ–è´¨é‡æŒ‡æ ‡",
                "timestamp": datetime.now().isoformat()
            }
        
        # è®°å½•å†å²
        self._record_quality_history(metrics)
        
        # åˆ†æè¶‹åŠ¿
        trends = {}
        if include_trends:
            trends = self.analyze_quality_trends(metrics)
        
        # ç”Ÿæˆå‘Šè­¦
        alerts = []
        if include_alerts:
            alerts = self.generate_quality_alerts(metrics)
            self._save_alerts(alerts)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_languages = len(metrics)
        avg_quality = sum(m.quality_score for m in metrics.values()) / total_languages
        avg_completeness = sum(m.completeness_score for m in metrics.values()) / total_languages
        avg_consistency = sum(m.consistency_score for m in metrics.values()) / total_languages
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = {}
        try:
            cache_stats = self.cache.get_cache_statistics()
        except Exception:
            cache_stats = {"status": "unavailable"}
        
        report = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_languages": total_languages,
                "average_quality": round(avg_quality, 3),
                "average_completeness": round(avg_completeness, 3),
                "average_consistency": round(avg_consistency, 3),
                "total_alerts": len(alerts),
                "critical_alerts": len([a for a in alerts if a.severity == "critical"])
            },
            "metrics": {lang: asdict(metric) for lang, metric in metrics.items()},
            "cache_stats": cache_stats
        }
        
        if include_trends:
            report["trends"] = {lang: asdict(trend) for lang, trend in trends.items()}
        
        if include_alerts:
            report["alerts"] = [asdict(alert) for alert in alerts]
        
        return report
    
    def run_continuous_monitoring(self, interval_minutes: int = 60):
        """è¿è¡ŒæŒç»­ç›‘æ§"""
        print(f"ğŸ”„ å¯åŠ¨æŒç»­ç›‘æ§ï¼Œé—´éš” {interval_minutes} åˆ†é’Ÿ")
        
        try:
            while True:
                print(f"\nâ° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - è¿è¡Œè´¨é‡æ£€æŸ¥")
                
                report = self.generate_quality_report()
                
                if report["status"] == "success":
                    summary = report["summary"]
                    print(f"ğŸ“Š è´¨é‡æ‘˜è¦:")
                    print(f"   è¯­è¨€æ•°é‡: {summary['total_languages']}")
                    print(f"   å¹³å‡è´¨é‡: {summary['average_quality']:.3f}")
                    print(f"   å¹³å‡å®Œæ•´åº¦: {summary['average_completeness']:.3f}")
                    print(f"   å‘Šè­¦æ•°é‡: {summary['total_alerts']}")
                    
                    if summary['critical_alerts'] > 0:
                        print(f"ğŸš¨ å‘ç° {summary['critical_alerts']} ä¸ªä¸¥é‡å‘Šè­¦!")
                else:
                    print(f"âŒ è´¨é‡æ£€æŸ¥å¤±è´¥: {report.get('message', 'æœªçŸ¥é”™è¯¯')}")
                
                print(f"ğŸ˜´ ç­‰å¾… {interval_minutes} åˆ†é’Ÿ...")
                time.sleep(interval_minutes * 60)
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ç›‘æ§å·²åœæ­¢")
        except Exception as e:
            print(f"âŒ ç›‘æ§å¼‚å¸¸: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="SuperClaudeç¿»è¯‘è´¨é‡ç›‘æ§")
    parser.add_argument("action", choices=[
        "report", "monitor", "alerts", "config", "trends"
    ], help="æ“ä½œç±»å‹")
    
    parser.add_argument("--project-root", default=".",
                       help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--interval", type=int, default=60,
                       help="ç›‘æ§é—´éš”ï¼ˆåˆ†é’Ÿï¼‰")
    parser.add_argument("--format", choices=["json", "text"], default="json",
                       help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("--save-report", 
                       help="ä¿å­˜æŠ¥å‘Šåˆ°æŒ‡å®šæ–‡ä»¶")
    parser.add_argument("--no-trends", action="store_true",
                       help="ä¸åŒ…å«è¶‹åŠ¿åˆ†æ")
    parser.add_argument("--no-alerts", action="store_true",
                       help="ä¸åŒ…å«å‘Šè­¦ç”Ÿæˆ")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè´¨é‡ç›‘æ§å™¨
    monitor = TranslationQualityMonitor(args.project_root)
    
    if args.action == "report":
        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        report = monitor.generate_quality_report(
            include_trends=not args.no_trends,
            include_alerts=not args.no_alerts
        )
        
        if args.format == "json":
            output = json.dumps(report, ensure_ascii=False, indent=2)
        else:
            # æ–‡æœ¬æ ¼å¼è¾“å‡º
            if report["status"] == "success":
                summary = report["summary"]
                output = f"""ğŸ“Š SuperClaudeç¿»è¯‘è´¨é‡æŠ¥å‘Š

ğŸŒ æ€»ä½“æ¦‚å†µ:
   æ”¯æŒè¯­è¨€: {summary['total_languages']} ç§
   å¹³å‡è´¨é‡: {summary['average_quality']:.3f}
   å¹³å‡å®Œæ•´åº¦: {summary['average_completeness']:.1%}
   å¹³å‡ä¸€è‡´æ€§: {summary['average_consistency']:.3f}

âš ï¸ å‘Šè­¦ç»Ÿè®¡:
   æ€»å‘Šè­¦æ•°: {summary['total_alerts']}
   ä¸¥é‡å‘Šè­¦: {summary['critical_alerts']}

ğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:"""
                
                for lang, metric in report["metrics"].items():
                    output += f"""
   {lang}: è´¨é‡{metric['quality_score']:.3f} å®Œæ•´åº¦{metric['completeness_score']:.1%}"""
            else:
                output = f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report.get('message', 'æœªçŸ¥é”™è¯¯')}"
        
        print(output)
        
        # ä¿å­˜æŠ¥å‘Š
        if args.save_report:
            try:
                with open(args.save_report, 'w', encoding='utf-8') as f:
                    if args.format == "json":
                        json.dump(report, f, ensure_ascii=False, indent=2)
                    else:
                        f.write(output)
                print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.save_report}")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    elif args.action == "monitor":
        # è¿è¡ŒæŒç»­ç›‘æ§
        monitor.run_continuous_monitoring(args.interval)
    
    elif args.action == "alerts":
        # æ˜¾ç¤ºå‘Šè­¦
        if monitor.alerts_file.exists():
            with open(monitor.alerts_file, 'r', encoding='utf-8') as f:
                alerts = json.load(f)
                print(json.dumps(alerts, ensure_ascii=False, indent=2))
        else:
            print("ğŸ“­ æš‚æ— å‘Šè­¦è®°å½•")
    
    elif args.action == "config":
        # æ˜¾ç¤ºé…ç½®
        print("âš™ï¸ å½“å‰ç›‘æ§é…ç½®:")
        print(json.dumps(monitor.config, ensure_ascii=False, indent=2))
    
    elif args.action == "trends":
        # è¶‹åŠ¿åˆ†æ
        metrics = monitor.collect_quality_metrics()
        trends = monitor.analyze_quality_trends(metrics)
        
        print("ğŸ“ˆ è´¨é‡è¶‹åŠ¿åˆ†æ:")
        for lang, trend in trends.items():
            print(f"\n{lang}:")
            print(f"   è¶‹åŠ¿æ–¹å‘: {trend.trend_direction}")
            print(f"   è¶‹åŠ¿å¼ºåº¦: {trend.trend_strength:.3f}")
            print(f"   å»ºè®®: {trend.recommendation}")


if __name__ == "__main__":
    main()