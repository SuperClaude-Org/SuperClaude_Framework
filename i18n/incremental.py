#!/usr/bin/env python3
"""
SuperClaudeå¢é‡ç¿»è¯‘ç®¡ç†å™¨ / SuperClaude Incremental Translation Manager
æ™ºèƒ½æ£€æµ‹å†…å®¹å˜æ›´ï¼Œåªç¿»è¯‘å˜æ›´éƒ¨åˆ†ï¼Œå¤§å¹…é™ä½ç¿»è¯‘æˆæœ¬ / Intelligently detects content changes, translates only changed parts, significantly reducing translation costs
"""

import os
import json
import hashlib
import asyncio
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime

from .extractor import SuperClaudeContentExtractor
from .cache import TranslationCache
from .translation_engine import TranslationEngineManager, create_translation_manager


@dataclass
class ContentChange:
    """å†…å®¹å˜æ›´è®°å½• / Content change record"""
    key: str                    # å†…å®¹é”®
    content_type: str          # å†…å®¹ç±»å‹ (commands, personas, ui, etc.)
    old_text: Optional[str]    # åŸæ–‡æœ¬
    new_text: str              # æ–°æ–‡æœ¬
    change_type: str           # å˜æ›´ç±»å‹: added, modified, deleted
    file_source: Optional[str] # æºæ–‡ä»¶
    hash_old: Optional[str]    # æ—§å†…å®¹å“ˆå¸Œ
    hash_new: str              # æ–°å†…å®¹å“ˆå¸Œ


@dataclass
class TranslationMemory:
    """ç¿»è¯‘è®°å¿†åº“æ¡ç›®"""
    source_text: str
    target_text: str
    source_lang: str
    target_lang: str
    content_type: str
    confidence: float
    last_used: str
    usage_count: int
    translation_engine: str


class IncrementalTranslationManager:
    """å¢é‡ç¿»è¯‘ç®¡ç†å™¨"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.data_dir = self.project_root / ".superclaude" / "incremental"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # çŠ¶æ€æ–‡ä»¶
        self.content_snapshot_file = self.data_dir / "content_snapshot.json"
        self.translation_memory_file = self.data_dir / "translation_memory.json"
        self.incremental_log_file = self.data_dir / "incremental_log.jsonl"
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.extractor = SuperClaudeContentExtractor(str(self.project_root))
        self.cache = TranslationCache()
        self.translation_manager = None
        
        # ç¿»è¯‘è®°å¿†åº“
        self.translation_memory = self._load_translation_memory()
        
        # æ”¯æŒçš„è¯­è¨€
        self.target_languages = [
            "zh_CN", "zh_TW", "ja_JP", "ko_KR", "ru_RU",
            "es_ES", "de_DE", "fr_FR", "ar_SA"
        ]
    
    def _initialize_translation_manager(self):
        """åˆå§‹åŒ–ç¿»è¯‘ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self.translation_manager is None:
            try:
                self.translation_manager = create_translation_manager()
            except Exception as e:
                print(f"âš ï¸ ç¿»è¯‘ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ / Translation manager initialization failed: {e}")
                self.translation_manager = None
    
    def _calculate_content_hash(self, content: Dict[str, Dict[str, str]]) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        content_str = json.dumps(content, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(content_str.encode('utf-8')).hexdigest()
    
    def _load_content_snapshot(self) -> Dict[str, Any]:
        """åŠ è½½å†…å®¹å¿«ç…§"""
        if not self.content_snapshot_file.exists():
            return {}
        
        try:
            with open(self.content_snapshot_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å†…å®¹å¿«ç…§å¤±è´¥ / Failed to load content snapshot: {e}")
            return {}
    
    def _save_content_snapshot(self, snapshot: Dict[str, Any]):
        """ä¿å­˜å†…å®¹å¿«ç…§"""
        try:
            with open(self.content_snapshot_file, 'w', encoding='utf-8') as f:
                json.dump(snapshot, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å†…å®¹å¿«ç…§å¤±è´¥ / Failed to save content snapshot: {e}")
    
    def _load_translation_memory(self) -> Dict[str, TranslationMemory]:
        """åŠ è½½ç¿»è¯‘è®°å¿†åº“"""
        if not self.translation_memory_file.exists():
            return {}
        
        try:
            with open(self.translation_memory_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            memory = {}
            for key, entry_data in data.items():
                memory[key] = TranslationMemory(**entry_data)
            
            return memory
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¿»è¯‘è®°å¿†åº“å¤±è´¥ / Failed to load translation memory: {e}")
            return {}
    
    def _save_translation_memory(self):
        """ä¿å­˜ç¿»è¯‘è®°å¿†åº“"""
        try:
            data = {}
            for key, memory_entry in self.translation_memory.items():
                data[key] = asdict(memory_entry)
            
            with open(self.translation_memory_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¿»è¯‘è®°å¿†åº“å¤±è´¥ / Failed to save translation memory: {e}")
    
    def _generate_memory_key(self, source_text: str, target_lang: str, content_type: str) -> str:
        """ç”Ÿæˆç¿»è¯‘è®°å¿†åº“é”®"""
        content = f"{source_text}|{target_lang}|{content_type}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _add_to_translation_memory(self, source_text: str, target_text: str, 
                                  target_lang: str, content_type: str,
                                  confidence: float = 1.0, engine: str = "unknown"):
        """æ·»åŠ åˆ°ç¿»è¯‘è®°å¿†åº“"""
        memory_key = self._generate_memory_key(source_text, target_lang, content_type)
        
        if memory_key in self.translation_memory:
            # æ›´æ–°ç°æœ‰æ¡ç›®
            memory_entry = self.translation_memory[memory_key]
            memory_entry.target_text = target_text
            memory_entry.confidence = confidence
            memory_entry.last_used = datetime.now().isoformat()
            memory_entry.usage_count += 1
            memory_entry.translation_engine = engine
        else:
            # åˆ›å»ºæ–°æ¡ç›®
            self.translation_memory[memory_key] = TranslationMemory(
                source_text=source_text,
                target_text=target_text,
                source_lang="en",
                target_lang=target_lang,
                content_type=content_type,
                confidence=confidence,
                last_used=datetime.now().isoformat(),
                usage_count=1,
                translation_engine=engine
            )
    
    def _get_from_translation_memory(self, source_text: str, target_lang: str, 
                                   content_type: str) -> Optional[str]:
        """ä»ç¿»è¯‘è®°å¿†åº“è·å–ç¿»è¯‘"""
        memory_key = self._generate_memory_key(source_text, target_lang, content_type)
        
        if memory_key in self.translation_memory:
            memory_entry = self.translation_memory[memory_key]
            memory_entry.last_used = datetime.now().isoformat()
            memory_entry.usage_count += 1
            return memory_entry.target_text
        
        return None
    
    def detect_content_changes(self) -> List[ContentChange]:
        """æ£€æµ‹å†…å®¹å˜æ›´"""
        print("ğŸ” æ£€æµ‹å†…å®¹å˜æ›´... / Detecting content changes...")
        
        # æå–å½“å‰å†…å®¹
        current_content = self.extractor.extract_all_content()
        current_hash = self._calculate_content_hash(current_content)
        
        # åŠ è½½å†å²å¿«ç…§
        old_snapshot = self._load_content_snapshot()
        old_content = old_snapshot.get("content", {})
        old_hash = old_snapshot.get("content_hash", "")
        
        changes = []
        
        # å¦‚æœæ•´ä½“å“ˆå¸Œç›¸åŒï¼Œæ²¡æœ‰å˜æ›´
        if current_hash == old_hash:
            print("âœ… æœªæ£€æµ‹åˆ°å†…å®¹å˜æ›´ / No content changes detected")
            return changes
        
        print(f"ğŸ“Š æ£€æµ‹åˆ°å†…å®¹å˜æ›´ / Content changes detected (å“ˆå¸Œ / hash: {old_hash[:8]} -> {current_hash[:8]})")
        
        # è¯¦ç»†å¯¹æ¯”æ¯ä¸ªå†…å®¹ç±»å‹å’Œé”®
        all_content_types = set(current_content.keys()) | set(old_content.keys())
        
        for content_type in all_content_types:
            current_section = current_content.get(content_type, {})
            old_section = old_content.get(content_type, {})
            
            all_keys = set(current_section.keys()) | set(old_section.keys())
            
            for key in all_keys:
                current_text = current_section.get(key)
                old_text = old_section.get(key)
                
                if current_text != old_text:
                    if current_text is None:
                        # åˆ é™¤çš„å†…å®¹
                        change = ContentChange(
                            key=key,
                            content_type=content_type,
                            old_text=old_text,
                            new_text="",
                            change_type="deleted",
                            file_source=None,
                            hash_old=hashlib.md5(old_text.encode()).hexdigest() if old_text else None,
                            hash_new=""
                        )
                    elif old_text is None:
                        # æ–°å¢çš„å†…å®¹
                        change = ContentChange(
                            key=key,
                            content_type=content_type,
                            old_text=None,
                            new_text=current_text,
                            change_type="added",
                            file_source=None,
                            hash_old=None,
                            hash_new=hashlib.md5(current_text.encode()).hexdigest()
                        )
                    else:
                        # ä¿®æ”¹çš„å†…å®¹
                        change = ContentChange(
                            key=key,
                            content_type=content_type,
                            old_text=old_text,
                            new_text=current_text,
                            change_type="modified",
                            file_source=None,
                            hash_old=hashlib.md5(old_text.encode()).hexdigest(),
                            hash_new=hashlib.md5(current_text.encode()).hexdigest()
                        )
                    
                    changes.append(change)
        
        print(f"ğŸ“‹ æ£€æµ‹åˆ° {len(changes)} é¡¹å†…å®¹å˜æ›´ / Detected {len(changes)} content changes")
        
        # æŒ‰å˜æ›´ç±»å‹åˆ†ç»„æ˜¾ç¤º
        change_summary = {}
        for change in changes:
            change_type = change.change_type
            change_summary[change_type] = change_summary.get(change_type, 0) + 1
        
        for change_type, count in change_summary.items():
            print(f"   {change_type}: {count} é¡¹")
        
        # ä¿å­˜æ–°çš„å¿«ç…§
        new_snapshot = {
            "content": current_content,
            "content_hash": current_hash,
            "timestamp": datetime.now().isoformat(),
            "changes_detected": len(changes)
        }
        self._save_content_snapshot(new_snapshot)
        
        return changes
    
    async def translate_changes(self, changes: List[ContentChange], 
                              target_languages: List[str] = None) -> Dict[str, Any]:
        """ç¿»è¯‘å˜æ›´å†…å®¹"""
        if not changes:
            return {"status": "success", "message": "æ— å˜æ›´éœ€è¦ç¿»è¯‘", "translations": 0}
        
        if target_languages is None:
            target_languages = self.target_languages
        
        print(f"ğŸš€ å¼€å§‹å¢é‡ç¿»è¯‘ / Starting incremental translation: {len(changes)} é¡¹å˜æ›´ / changes, {len(target_languages)} ç§è¯­è¨€ / languages")
        
        # åˆå§‹åŒ–ç¿»è¯‘ç®¡ç†å™¨
        self._initialize_translation_manager()
        if self.translation_manager is None:
            return {"status": "error", "message": "ç¿»è¯‘ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥"}
        
        translation_results = {}
        total_translations = 0
        cache_hits = 0
        
        for target_lang in target_languages:
            print(f"\nğŸŒ ç¿»è¯‘åˆ° / Translating to {target_lang}...")
            translation_results[target_lang] = {}
            
            for change in changes:
                if change.change_type == "deleted":
                    continue  # è·³è¿‡åˆ é™¤çš„å†…å®¹
                
                source_text = change.new_text
                content_type = change.content_type
                key = change.key
                
                # é¦–å…ˆå°è¯•ç¿»è¯‘è®°å¿†åº“
                cached_translation = self._get_from_translation_memory(
                    source_text, target_lang, content_type
                )
                
                if cached_translation:
                    translation_results[target_lang][key] = cached_translation
                    cache_hits += 1
                    continue
                
                # ç„¶åå°è¯•ç¿»è¯‘ç¼“å­˜
                cached_translation = self.cache.get_translation(
                    source_text, target_lang, content_type
                )
                
                if cached_translation:
                    translation_results[target_lang][key] = cached_translation
                    # æ·»åŠ åˆ°ç¿»è¯‘è®°å¿†åº“
                    self._add_to_translation_memory(
                        source_text, cached_translation, target_lang, 
                        content_type, confidence=0.9, engine="cache"
                    )
                    cache_hits += 1
                    continue
                
                # éœ€è¦æ–°ç¿»è¯‘
                try:
                    print(f"   ğŸ”„ ç¿»è¯‘ / Translating {content_type}.{key}...")
                    
                    # è·å–ç¿»è¯‘å¼•æ“
                    engine = self.translation_manager.get_engine()
                    context = engine.contexts.get(content_type, engine.contexts["commands"])
                    
                    # æ‰§è¡Œç¿»è¯‘
                    result = await self.translation_manager.translate_with_context(
                        source_text, target_lang, context
                    )
                    
                    translation_results[target_lang][key] = result.target
                    
                    # å­˜å‚¨åˆ°ç¼“å­˜å’Œè®°å¿†åº“
                    self.cache.store_translation(
                        source_text, result.target, target_lang, content_type,
                        confidence=result.confidence, cost=result.cost_estimate
                    )
                    
                    self._add_to_translation_memory(
                        source_text, result.target, target_lang, content_type,
                        confidence=result.confidence, engine="live"
                    )
                    
                    total_translations += 1
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    print(f"âš ï¸ ç¿»è¯‘å¤±è´¥ / Translation failed {content_type}.{key}: {e}")
                    # ä½¿ç”¨åŸæ–‡ä½œä¸ºfallback
                    translation_results[target_lang][key] = source_text
        
        # ä¿å­˜ç¿»è¯‘è®°å¿†åº“
        self._save_translation_memory()
        
        # è®°å½•å¢é‡ç¿»è¯‘æ—¥å¿—
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "changes_count": len(changes),
            "target_languages": target_languages,
            "total_translations": total_translations,
            "cache_hits": cache_hits,
            "cache_hit_rate": cache_hits / (total_translations + cache_hits) if (total_translations + cache_hits) > 0 else 0
        }
        
        try:
            with open(self.incremental_log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"âš ï¸ è®°å½•å¢é‡ç¿»è¯‘æ—¥å¿—å¤±è´¥ / Failed to log incremental translation: {e}")
        
        print(f"\nâœ… å¢é‡ç¿»è¯‘å®Œæˆ / Incremental translation completed!")
        print(f"   æ–°ç¿»è¯‘ / New translations: {total_translations} é¡¹ / items")
        print(f"   ç¼“å­˜å‘½ä¸­ / Cache hits: {cache_hits} é¡¹ / items")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡ / Cache hit rate: {log_entry['cache_hit_rate']:.1%}")
        
        return {
            "status": "success",
            "message": f"å¢é‡ç¿»è¯‘å®Œæˆ: {total_translations} é¡¹æ–°ç¿»è¯‘, {cache_hits} é¡¹ç¼“å­˜å‘½ä¸­",
            "translations": total_translations,
            "cache_hits": cache_hits,
            "cache_hit_rate": log_entry['cache_hit_rate'],
            "results": translation_results
        }
    
    def get_incremental_statistics(self) -> Dict[str, Any]:
        """è·å–å¢é‡ç¿»è¯‘ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "translation_memory_size": len(self.translation_memory),
            "cache_stats": self.cache.get_cache_statistics(),
            "last_snapshot": None,
            "total_incremental_runs": 0,
            "total_translations": 0,
            "average_cache_hit_rate": 0.0
        }
        
        # è¯»å–å¿«ç…§ä¿¡æ¯
        if self.content_snapshot_file.exists():
            try:
                with open(self.content_snapshot_file, 'r', encoding='utf-8') as f:
                    snapshot = json.load(f)
                    stats["last_snapshot"] = {
                        "timestamp": snapshot.get("timestamp"),
                        "content_hash": snapshot.get("content_hash", "")[:8],
                        "changes_detected": snapshot.get("changes_detected", 0)
                    }
            except Exception:
                pass
        
        # è¯»å–å¢é‡ç¿»è¯‘æ—¥å¿—ç»Ÿè®¡
        if self.incremental_log_file.exists():
            try:
                total_runs = 0
                total_translations = 0
                total_cache_hits = 0
                
                with open(self.incremental_log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            log_entry = json.loads(line)
                            total_runs += 1
                            total_translations += log_entry.get("total_translations", 0)
                            total_cache_hits += log_entry.get("cache_hits", 0)
                
                stats["total_incremental_runs"] = total_runs
                stats["total_translations"] = total_translations
                
                if total_translations + total_cache_hits > 0:
                    stats["average_cache_hit_rate"] = total_cache_hits / (total_translations + total_cache_hits)
                    
            except Exception:
                pass
        
        return stats
    
    def reset_incremental_data(self):
        """é‡ç½®å¢é‡ç¿»è¯‘æ•°æ®ï¼ˆç”¨äºæµ‹è¯•æˆ–é‡æ–°å¼€å§‹ï¼‰"""
        print("ğŸ—‘ï¸ é‡ç½®å¢é‡ç¿»è¯‘æ•°æ®...")
        
        files_to_remove = [
            self.content_snapshot_file,
            self.translation_memory_file,
            self.incremental_log_file
        ]
        
        for file_path in files_to_remove:
            if file_path.exists():
                try:
                    file_path.unlink()
                    print(f"âœ… å·²åˆ é™¤: {file_path}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤å¤±è´¥ {file_path}: {e}")
        
        # é‡ç½®å†…å­˜ä¸­çš„æ•°æ®
        self.translation_memory = {}
        
        print("âœ… å¢é‡ç¿»è¯‘æ•°æ®é‡ç½®å®Œæˆ")


async def main():
    """æµ‹è¯•å¢é‡ç¿»è¯‘ç®¡ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•å¢é‡ç¿»è¯‘ç®¡ç†å™¨")
    print("=" * 50)
    
    manager = IncrementalTranslationManager()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_incremental_statistics()
    print("ğŸ“Š å½“å‰çŠ¶æ€:")
    print(json.dumps(stats, ensure_ascii=False, indent=2))
    
    # æ£€æµ‹å˜æ›´
    changes = manager.detect_content_changes()
    
    if changes:
        print(f"\nğŸ“‹ æ£€æµ‹åˆ° {len(changes)} é¡¹å˜æ›´:")
        for i, change in enumerate(changes[:5], 1):  # åªæ˜¾ç¤ºå‰5é¡¹
            print(f"   {i}. {change.change_type}: {change.content_type}.{change.key}")
            if change.new_text and len(change.new_text) < 100:
                print(f"      -> {change.new_text}")
        
        if len(changes) > 5:
            print(f"   ... è¿˜æœ‰ {len(changes) - 5} é¡¹å˜æ›´")
        
        # è¯¢é—®æ˜¯å¦æ‰§è¡Œç¿»è¯‘ï¼ˆåœ¨å®é™…ä½¿ç”¨ä¸­ä¼šè‡ªåŠ¨æ‰§è¡Œï¼‰
        print(f"\næ˜¯å¦æ‰§è¡Œå¢é‡ç¿»è¯‘ï¼Ÿ(ä»…æµ‹è¯•æ¨¡å¼)")
        # åœ¨è¿™é‡Œå¯ä»¥æ·»åŠ ç”¨æˆ·ç¡®è®¤é€»è¾‘
        
    else:
        print("âœ… æ— å˜æ›´éœ€è¦ç¿»è¯‘")


if __name__ == "__main__":
    asyncio.run(main())